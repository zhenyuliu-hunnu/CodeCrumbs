{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcJK3kXl--c3"
      },
      "source": [
        "# EECS 498-007/598-005 Assignment 1-1: PyTorch 101\n",
        "\n",
        "Before we start, please put your name and UMID in following format\n",
        "\n",
        ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sA2iBcm_cPb"
      },
      "source": [
        "**Your Answer:**   \n",
        "Hello WORLD, #XXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc83ETI1a3o9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Python 3 and [PyTorch](https://pytorch.org/) will be used throughout the semseter, so it is important to be familiar with them. This material in this notebook draws from http://cs231n.github.io/python-numpy-tutorial/ and https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb. This material focuses mainly on PyTorch.\n",
        "\n",
        "This notebook will walk you through many of the important features of PyTorch that you will need to use throughout the semester. In some cells you will see code blocks that look like this:\n",
        "\n",
        "```python\n",
        "##############################################################################\n",
        "# TODO: Write the equation for a line\n",
        "##############################################################################\n",
        "pass\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "```\n",
        "\n",
        "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
        "\n",
        "```python\n",
        "##############################################################################\n",
        "# TODO: Instructions for what you need to do\n",
        "##############################################################################\n",
        "y = m * x + b\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "```\n",
        "\n",
        "When completing the notebook, please adhere to the following rules:\n",
        "- Do not write or modify any code outside of code blocks\n",
        "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
        "- Run all cells before submitting. You will only get credit for code that has been run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrEwOpXb9Gh"
      },
      "source": [
        "# Python 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAKwfCs_mK3d"
      },
      "source": [
        "If you're unfamiliar with Python 3, here are some of the most common changes from Python 2 to look out for.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjosrOn8mOMV"
      },
      "source": [
        "### Print is a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O41SjFuamR7d"
      },
      "outputs": [],
      "source": [
        "print(\"Hello!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEh1swLBmQN-"
      },
      "source": [
        "Without parentheses, printing will not work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgPaSNS2mVPn"
      },
      "source": [
        "### Floating point division by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQKlRZ8KmYDl"
      },
      "outputs": [],
      "source": [
        "5 / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOmfK0WWmb2V"
      },
      "source": [
        "To do integer division, we use two backslashes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUg1MjiPmgNX"
      },
      "outputs": [],
      "source": [
        "5 // 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeH5501nmh7W"
      },
      "source": [
        "### No xrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wNKyyilmkMy"
      },
      "source": [
        "The xrange from Python 2 is now merged into \"range\" for Python 3 and there is no xrange in Python 3. In Python 3, range(3) does not create a list of 3 elements as it would in Python 2, rather just creates a more memory efficient iterator.\n",
        "\n",
        "Hence,  \n",
        "xrange in Python 3: Does not exist  \n",
        "range in Python 3: Has very similar behavior to Python 2's xrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP8Dk9PAmnQh"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SKbKDgLmqd-"
      },
      "outputs": [],
      "source": [
        "range(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm_VcW3VmsSD"
      },
      "outputs": [],
      "source": [
        "# If need be, can use the following to get a similar behavior to Python 2's range:\n",
        "print(list(range(3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6UsRPdHfFMZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MEmHrgBsgX4"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3e_Nux0siHo"
      },
      "source": [
        "[PyTorch](https://pytorch.org/) is an open source machine learning framework. At its core, PyTorch provides a few key features:\n",
        "\n",
        "- A multidimensional **Tensor** object, similar to [numpy](https://numpy.org/) but with GPU accelleration.\n",
        "- An optimized **autograd** engine for automatically computing derivatives\n",
        "- A clean, modular API for building and deploying **deep learning models**\n",
        "\n",
        "We will use PyTorch for all programming assignments throughout the semester. This notebook will focus on the **Tensor API**, as it is the main part of PyTorch that we will use for the first few assignments.\n",
        "\n",
        "You can find more information about PyTorch by following one of the [oficial tutorials](https://pytorch.org/tutorials/) or by [reading the documentation](https://pytorch.org/docs/1.1.0/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiO3_y-vKQ9"
      },
      "source": [
        "To use PyTorch, we first need to import the `torch` package.\n",
        "\n",
        "We also check the version; the assignments in this course will use PyTorch verion 1.1.0, since this is the default version in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sydFm14itrqq",
        "outputId": "b2c63136-9fb1-4fcf-dc28-7f2e8dd557c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrBSx6hYu8ca"
      },
      "source": [
        "## Tensor Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWagwmXuvIle"
      },
      "source": [
        "### Creating and Accessing tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf_SY4RzvAh_"
      },
      "source": [
        "`torch` **张量(tensor)** 是一个多维值网格，所有值类型相同，并由非负整数的元组索引。维数的数量是张量的 **秩(rank)**；张量的 **形状(shape)** 是一个整数元组，给出数组在每个维度上的大小。\n",
        "\n",
        "我们可以从嵌套的 Python 列表初始化 `torch` 张量。我们可以使用方括号访问或修改 PyTorch 张量的元素。\n",
        "\n",
        "从 PyTorch 张量中访问一个元素会返回一个 PyTorch 标量；我们可以使用 `.item()` 方法将其转换为 Python 标量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpwfVUvPu_lF",
        "outputId": "4628838e-70df-4279-d9a4-9918d88dd2d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a:\n",
            "tensor([1, 2, 3])\n",
            "type(a):  <class 'torch.Tensor'>\n",
            "rank of a:  1\n",
            "a.shape:  torch.Size([3])\n",
            "\n",
            "a[0]:  tensor(1)\n",
            "type(a[0]):  <class 'torch.Tensor'>\n",
            "type(a[0].item()):  <class 'int'>\n",
            "\n",
            "a after mutating:\n",
            "tensor([ 1, 10,  3])\n"
          ]
        }
      ],
      "source": [
        "# Create a rank 1 tensor from a Python list\n",
        "a = torch.tensor([1, 2, 3])\n",
        "print('Here is a:')\n",
        "print(a)\n",
        "print('type(a): ', type(a))\n",
        "print('rank of a: ', a.dim())\n",
        "print('a.shape: ', a.shape)\n",
        "\n",
        "# Access elements using square brackets\n",
        "print()\n",
        "print('a[0]: ', a[0])\n",
        "print('type(a[0]): ', type(a[0]))\n",
        "print('type(a[0].item()): ', type(a[0].item()))\n",
        "\n",
        "# Mutate elements using square brackets\n",
        "a[1] = 10\n",
        "print()\n",
        "print('a after mutating:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZq4zsnLEgXH"
      },
      "source": [
        "上面的例子显示了一个一维张量；我们同样可以创建具有二维或更多维度的张量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TcvHxpTFUcL",
        "outputId": "5302a9c8-0daf-40c7-b844-c6c102647fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is b:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 5]])\n",
            "rank of b: 2\n",
            "b.shape:  torch.Size([2, 3])\n",
            "\n",
            "b[0, 1]: tensor(2)\n",
            "b[1, 2]: tensor(5)\n",
            "\n",
            "b after mutating:\n",
            "tensor([[  1,   2,   3],\n",
            "        [  4, 100,   5]])\n"
          ]
        }
      ],
      "source": [
        "# Create a two-dimensional tensor\n",
        "b = torch.tensor([[1, 2, 3], [4, 5, 5]])\n",
        "print('Here is b:')\n",
        "print(b)\n",
        "print('rank of b:', b.dim())\n",
        "print('b.shape: ', b.shape)\n",
        "\n",
        "# Access elements from a multidimensional tensor\n",
        "print()\n",
        "print('b[0, 1]:', b[0, 1])\n",
        "print('b[1, 2]:', b[1, 2])\n",
        "\n",
        "# Mutate elements of a multidimensional tensor\n",
        "b[1, 1] = 100\n",
        "print()\n",
        "print('b after mutating:')\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBOsvh53GXa8"
      },
      "source": [
        "Now it's your turn:\n",
        "\n",
        "1. Construct a tensor `c` of shape `(3, 2)` filled with zeros by initializing from nested Python lists.\n",
        "2. Then set element `(0, 1)` to 10, and element `(1, 0)` to 100:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TwkjP0GwIPG",
        "outputId": "4accbe1a-9f0b-429d-f750-f5fc91421c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c is a tensor:  True\n",
            "Correct shape:  True\n",
            "All zeros:  True\n",
            "\n",
            "After mutating:\n",
            "Correct shape:  True\n",
            "c[0, 1] correct:  tensor(True)\n",
            "c[1, 0] correct:  tensor(True)\n",
            "Rest of c is still zero:  True\n"
          ]
        }
      ],
      "source": [
        "c = None\n",
        "################################################################################\n",
        "# TODO: Construct a tensor c filled with all zeros, initializing from nested   #\n",
        "# Python lists.                                                                #\n",
        "################################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "c = torch.tensor([[0,0],[0,0],[0,0]])\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "print('c is a tensor: ', torch.is_tensor(c))\n",
        "print('Correct shape: ', c.shape == (3, 2))\n",
        "print('All zeros: ', (c == 0).all().item() == 1)\n",
        "\n",
        "################################################################################\n",
        "# TODO: Set element (0, 1) of c to 10, and element (1, 0) of c to 100.         #\n",
        "################################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "c[0,1], c[1,0] = 10, 100\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "print('\\nAfter mutating:')\n",
        "print('Correct shape: ', c.shape == (3, 2))\n",
        "print('c[0, 1] correct: ', c[0, 1] == 10)\n",
        "print('c[1, 0] correct: ', c[1, 0] == 100)\n",
        "print('Rest of c is still zero: ', (c == 0).sum().item() == 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz_VDA3IvP33"
      },
      "source": [
        "### Tensor constructors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoAlslEdwV-k"
      },
      "source": [
        "PyTorch 提供了许多方便的方法来构建张量；这避免了使用 Python 列表的需要。例如：\n",
        "\n",
        "- [`torch.zeros`](https://pytorch.org/docs/1.1.0/torch.html#torch.zeros): 创建一个全为0的张量\n",
        "- [`torch.ones`](https://pytorch.org/docs/1.1.0/torch.html#torch.ones): 创建一个全为1的张量\n",
        "- [`torch.rand`](https://pytorch.org/docs/1.1.0/torch.html#torch.rand): 创建一个具有均匀随机数的张量\n",
        "\n",
        "You can find a full list of tensor creation operations [in the documentation](https://pytorch.org/docs/1.1.0/torch.html#creation-ops)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL6DXGXzxHBA",
        "outputId": "ee4cb8e1-a28a-447f-fd4d-607c80d3d702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor of zeros:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "tensor of ones:\n",
            "tensor([[1., 1.]])\n",
            "\n",
            "identity matrix:\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "\n",
            "random tensor:\n",
            "tensor([[0.4163, 0.9638, 0.6795, 0.4539, 0.1354],\n",
            "        [0.8309, 0.1014, 0.1678, 0.1838, 0.9939],\n",
            "        [0.1839, 0.3927, 0.9846, 0.1630, 0.1193],\n",
            "        [0.8057, 0.3750, 0.8874, 0.3205, 0.1990]])\n"
          ]
        }
      ],
      "source": [
        "# 全零张量\n",
        "a = torch.zeros(2, 3)\n",
        "print('tensor of zeros:')\n",
        "print(a)\n",
        "\n",
        "# 全1张量\n",
        "b = torch.ones(1, 2)\n",
        "print('\\ntensor of ones:')\n",
        "print(b)\n",
        "\n",
        "# 3x3 对角矩阵\n",
        "c = torch.eye(3)\n",
        "print('\\nidentity matrix:')\n",
        "print(c)\n",
        "\n",
        "# 随机矩阵\n",
        "d = torch.rand(4, 5)\n",
        "print('\\nrandom tensor:')\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9QuvWYxMsoK"
      },
      "source": [
        "轮到你了：使用张量创建函数创建一个形状为 (2, 3, 4) 的张量，全部填充为 7。\n",
        "\n",
        "Hint: [`torch.full`](https://pytorch.org/docs/1.1.0/torch.html#torch.full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_y7Z5I0NIaA",
        "outputId": "9ceffdd5-c2be-4be6-848f-f260f6724140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e is a tensor: True\n",
            "e has correct shape:  True\n",
            "e is filled with sevens:  True\n"
          ]
        }
      ],
      "source": [
        "e = None\n",
        "################################################################################\n",
        "# TODO: Create a tensor of shape (2, 3, 4) filled entirely with 7, stored in e #\n",
        "################################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "e = torch.full((2,3,4),7)\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "print('e is a tensor:', torch.is_tensor(e))\n",
        "print('e has correct shape: ', e.shape == (2, 3, 4))\n",
        "print('e is filled with sevens: ', (e == 7).all().item() == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz_hiJD33fu1"
      },
      "source": [
        "### 数据类型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG1xBunZ3ixx"
      },
      "source": [
        "在上面的例子中，您可能已经注意到我们的一些张量包含浮点值，而其他张量包含整数值。\n",
        "\n",
        "PyTorch 可以用 [各种类型的数值](https://pytorch.org/docs/1.1.0/tensor_attributes.html#torch-dtype) 构造张量。 当你创建一个张量时，PyTorch 会尝试猜测数据类型；构造张量的函数通常有一个 `dtype` 参数，你可以用它来显式指定数据类型。\n",
        "\n",
        "每个张量都有一个 `dtype` 属性，您可以用来检查其数据类型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vREVDf_n31Qz",
        "outputId": "1ab91d1a-4f74-425a-dc62-eed442d2c6d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dtype when torch chooses for us:\n",
            "List of integers: torch.int64\n",
            "List of floats: torch.float32\n",
            "Mixed list: torch.float32\n",
            "\n",
            "dtype when we force a datatype:\n",
            "32-bit float:  torch.float32\n",
            "32-bit integer:  torch.int32\n",
            "64-bit integer:  torch.int64\n",
            "\n",
            "torch.ones with different dtypes\n",
            "default dtype: torch.float32\n",
            "16-bit integer: torch.int16\n",
            "8-bit unsigned integer: torch.uint8\n"
          ]
        }
      ],
      "source": [
        "# 让 torch 自动选择\n",
        "x0 = torch.tensor([1, 2])   # List of integers\n",
        "x1 = torch.tensor([1., 2.]) # List of floats\n",
        "x2 = torch.tensor([1., 2])  # Mixed list\n",
        "print('dtype when torch chooses for us:')\n",
        "print('List of integers:', x0.dtype)\n",
        "print('List of floats:', x1.dtype)\n",
        "print('Mixed list:', x2.dtype)\n",
        "\n",
        "# 指定数据类型\n",
        "y0 = torch.tensor([1, 2], dtype=torch.float32)  # 32-bit float\n",
        "y1 = torch.tensor([1, 2], dtype=torch.int32)    # 32-bit (signed) integer\n",
        "y2 = torch.tensor([1, 2], dtype=torch.int64)    # 64-bit (signed) integer\n",
        "print('\\ndtype when we force a datatype:')\n",
        "print('32-bit float: ', y0.dtype)\n",
        "print('32-bit integer: ', y1.dtype)\n",
        "print('64-bit integer: ', y2.dtype)\n",
        "\n",
        "# 使用其他构建张量方法时指定数字类型\n",
        "z0 = torch.ones(1, 2)  # Let torch choose for us\n",
        "z1 = torch.ones(1, 2, dtype=torch.int16) # 16-bit (signed) integer\n",
        "z2 = torch.ones(1, 2, dtype=torch.uint8) # 8-bit (unsigned) integer\n",
        "print('\\ntorch.ones with different dtypes')\n",
        "print('default dtype:', z0.dtype)\n",
        "print('16-bit integer:', z1.dtype)\n",
        "print('8-bit unsigned integer:', z2.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2reBgQmx_x4"
      },
      "source": [
        "转换（**cast**）一个张量的数据类型可以使用[`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to)方法; 其他简便方法还有类似[`.float()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.float) （等价 `self.to(torch.float32)`）和 [`.long()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.long)（等价`self.to(torch.int64)`）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAMpwGsdyHAw",
        "outputId": "603e7f65-795b-4825-ffcf-34a178fb7a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x0: torch.int64\n",
            "x1: torch.float32\n",
            "x2: torch.float64\n",
            "x3: torch.float32\n",
            "x4: torch.float64\n"
          ]
        }
      ],
      "source": [
        "x0 = torch.eye(3, dtype=torch.int64)\n",
        "x1 = x0.float()  # Cast to 32-bit float\n",
        "x2 = x0.double() # Cast to 64-bit float\n",
        "x3 = x0.to(torch.float32) # Alternate way to cast to 32-bit float\n",
        "x4 = x0.to(torch.float64) # Alternate way to cast to 64-bit float\n",
        "print('x0:', x0.dtype)\n",
        "print('x1:', x1.dtype)\n",
        "print('x2:', x2.dtype)\n",
        "print('x3:', x3.dtype)\n",
        "print('x4:', x4.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2O8Atl1wMB7"
      },
      "source": [
        "PyTorch 提供了几种方法来创建与另一个张量具有相同数据类型的张量：\n",
        "\n",
        "- PyTorch 可以使用类似于 [`torch.new_zeros()`](https://pytorch.org/docs/1.1.0/torch.html#torch.zeros_like) 的方式创建指定与指定张量形状和数据类型相同的张量。\n",
        "- 张量对象具有实例方法，例如[`.new_zeros()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.new_zeros)，可以创建相同类型但形状可能不同的张量。\n",
        "- 对于张量实例，可以在 [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) 内将另一个张量作为参数, 将实例张量的数据类型与指定张量保持一致。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1APDsx54xV6p"
      },
      "outputs": [],
      "source": [
        "x0 = torch.eye(3, dtype=torch.float64)  # Shape (3, 3), dtype torch.float64\n",
        "x1 = torch.zeros_like(x0)               # Shape (3, 3), dtype torch.float64\n",
        "x2 = x0.new_zeros(4, 5)                 # Shape (4, 5), dtype torch.float64\n",
        "x3 = torch.ones(6, 7).to(x0)            # Shape (6, 7), dtype torch.float64)\n",
        "print('x0 shape is %r, dtype is %r' % (x0.shape, x0.dtype))\n",
        "print('x1 shape is %r, dtype is %r' % (x1.shape, x1.dtype))\n",
        "print('x2 shape is %r, dtype is %r' % (x2.shape, x2.dtype))\n",
        "print('x3 shape is %r, dtype is %r' % (x3.shape, x3.dtype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPuGPa0v4h_2"
      },
      "source": [
        "Your turn: Create a 64-bit floating-point tensor of shape (6,) (six-element vector) filled with evenly-spaced values between 10 and 20.\n",
        "\n",
        "Hint: [`torch.linspace`](https://pytorch.org/docs/stable/torch.html#torch.linspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qddo6C5Bgwcr",
        "outputId": "20cff705-a7ae-487a-c5dc-75f752ed12af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x is a tensor:  True\n",
            "x has correct shape:  True\n",
            "x has correct dtype:  True\n",
            "x has correct valus:  True\n"
          ]
        }
      ],
      "source": [
        "x = None\n",
        "##############################################################################\n",
        "# TODO: Make x contain a six-element vector of 64-bit floating-bit values,   #\n",
        "# evenly spaced between 10 and 20.                                           #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "x = torch.linspace(start=10, end=20, steps=6).to(torch.float64)\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('x is a tensor: ', torch.is_tensor(x))\n",
        "print('x has correct shape: ', x.shape == (6,))\n",
        "print('x has correct dtype: ', x.dtype == torch.float64)\n",
        "y = [10, 12, 14, 16, 18, 20]\n",
        "correct_vals = all(a.item() == b for a, b in zip(x, y))\n",
        "print('x has correct valus: ', correct_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwJL3HVySvXn"
      },
      "source": [
        "尽管 PyTorch 提供了大量数字数据类型，但最常用的数据类型是\n",
        "\n",
        "- torch.float32`：标准浮点类型；用于存储可学习参数、网络激活等。几乎所有算术运算都使用这种类型。\n",
        "\n",
        "- `torch.int64`：通常用于存储指数\n",
        "\n",
        "- `torch.uint8`：通常用于存储布尔值，其中 0 表示假，1 表示真。\n",
        "\n",
        "- `torch.float16`：用于混合精度算术，通常在英伟达™（NVIDIA®）图形处理器上使用[Tensor Cores](https://www.nvidia.com/en-us/data-center/tensorcore/)。在本课程中，您不需要担心这种数据类型。\n",
        "\n",
        "请注意，PyTorch 1.2.0 版本引入了一个新的 `torch.bool` 数据类型，用于保存布尔值。然而，在早期版本（包括我们在本课程中使用的 1.1.0）中，您会看到 `torch.uint8` 被用来保存布尔值。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlANfnILvX3S"
      },
      "source": [
        "## 张量索引"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP4dRrHhyLO5"
      },
      "source": [
        "我们已经了解了如何获取和设置PyTorch张量的各个元素。PyTorch还提供了许多其他方法来索引张量。熟悉这些不同的选项可以轻松地修改张量的不同部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo-PoTWNvbba"
      },
      "source": [
        "### 切片索引"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUqTYvglyVLc"
      },
      "source": [
        "与 Python 列表和 numpy 数组类似，PyTorch 张量可以使用 `start:stop` 或 `start:stop:step` 语法进行**切片**。`stop` 索引始终是非包含性的：它是切片中不包含的第一个元素。\n",
        "\n",
        "起始和结束索引可以是负数，在这种情况下，它们从张量的末尾向后计数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEr5BzdUdCtZ",
        "outputId": "5345a171-ba75-4522-8e4d-791b36e6ae5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tensor([ 0, 11, 22, 33, 44, 55, 66])\n",
            "1 tensor([22, 33, 44])\n",
            "2 tensor([22, 33, 44, 55, 66])\n",
            "3 tensor([ 0, 11, 22, 33, 44])\n",
            "4 tensor([ 0, 11, 22, 33, 44, 55, 66])\n",
            "5 tensor([11, 33])\n",
            "6 tensor([ 0, 11, 22, 33, 44, 55])\n",
            "7 tensor([33, 55])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([0, 11, 22, 33, 44, 55, 66])\n",
        "print(0, a)        # (0) 原张量\n",
        "print(1, a[2:5])   # (1) 张量的第2至第4索引的元素\n",
        "print(2, a[2:])    # (2) 索引2及之后的元素\n",
        "print(3, a[:5])    # (3) 索引5之前的元素\n",
        "print(4, a[:])     # (4) 所有元素\n",
        "print(5, a[1:5:2]) # (5) 索引 1 至 5 之间的每第二个元素（含索引1）\n",
        "print(6, a[:-1])   # (6) 去除最后一个元素\n",
        "print(7, a[-4::2]) # (7) 从倒数第四个元素开始，每第二个元素"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrcr9PojgTS1"
      },
      "source": [
        "对于多维张量，您可以为张量的每个维度提供一个切片或整数，以提取不同类型的子张量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5fOdjTUyhNf",
        "outputId": "8231a94a-4a8a-4926-fa1c-13a2d44e73ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12]])\n",
            "shape:  torch.Size([3, 4])\n",
            "\n",
            "Single row:\n",
            "tensor([5, 6, 7, 8])\n",
            "tensor([5, 6, 7, 8])\n",
            "shape:  torch.Size([4])\n",
            "\n",
            "Single column:\n",
            "tensor([ 2,  6, 10])\n",
            "shape:  torch.Size([3])\n",
            "\n",
            "First two rows, last two columns:\n",
            "tensor([[2, 3, 4],\n",
            "        [6, 7, 8]])\n",
            "shape:  torch.Size([2, 3])\n",
            "\n",
            "Every other row, middle columns:\n",
            "tensor([[ 2,  3],\n",
            "        [10, 11]])\n",
            "shape:  torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "# [[ 1  2  3  4]\n",
        "#  [ 5  6  7  8]\n",
        "#  [ 9 10 11 12]]\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "print('shape: ', a.shape)\n",
        "\n",
        "# 提取第一行\n",
        "print('\\nSingle row:')\n",
        "print(a[1, :])\n",
        "print(a[1])  # Gives the same result; we can omit : for trailing dimensions\n",
        "print('shape: ', a[1].shape)\n",
        "\n",
        "#提取第一列\n",
        "print('\\nSingle column:')\n",
        "print(a[:, 1])\n",
        "print('shape: ', a[:, 1].shape)\n",
        "\n",
        "# 提取前两行的后三列\n",
        "print('\\nFirst two rows, last two columns:')\n",
        "print(a[:2, -3:])\n",
        "print('shape: ', a[:2, -3:].shape)\n",
        "\n",
        "# 获取每隔一行，以及索引为1和2的列。\n",
        "print('\\nEvery other row, middle columns:')\n",
        "print(a[::2, 1:3])\n",
        "print('shape: ', a[::2, 1:3].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOsR8Pdertku"
      },
      "source": [
        "访问张量的单行或单列有两种常见方法：使用单个数值会将秩减少1，使用长度为1的切片会保持相同的秩。请注意，这与MATLAB的行为不同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1kHcc5jsF-c",
        "outputId": "00d84d23-c06f-4fa3-c8a0-ad173839e64f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12]])\n",
            "\n",
            "Two ways of accessing a single row:\n",
            "tensor([5, 6, 7, 8]) torch.Size([4])\n",
            "tensor([[5, 6, 7, 8]]) torch.Size([1, 4])\n",
            "\n",
            "Two ways of accessing a single column:\n",
            "tensor([ 2,  6, 10]) torch.Size([3])\n",
            "tensor([[ 2],\n",
            "        [ 6],\n",
            "        [10]]) torch.Size([3, 1])\n"
          ]
        }
      ],
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor')\n",
        "print(a)\n",
        "\n",
        "row_r1 = a[1, :]    # Rank 1 view of the second row of a\n",
        "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
        "print('\\nTwo ways of accessing a single row:')\n",
        "print(row_r1, row_r1.shape)\n",
        "print(row_r2, row_r2.shape)\n",
        "\n",
        "# 当访问列时，我们可以做出同样的区分：\n",
        "col_r1 = a[:, 1] # 行向量\n",
        "col_r2 = a[:, 1:2] # 列向量\n",
        "print('\\nTwo ways of accessing a single column:')\n",
        "print(col_r1, col_r1.shape)\n",
        "print(col_r2, col_r2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk625fJfyxV8"
      },
      "source": [
        "对张量进行切片会返回相同数据的 **视图（view）** ，因此修改它也会修改原始张量。为了避免这种情况，可以使用 `clone()` 方法来复制一个张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXbikYPwyxGA",
        "outputId": "1a91e429-f853-40d8-fc34-1e1ad5727156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before mutating:\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "tensor([2, 3, 4])\n",
            "tensor([2, 3, 4])\n",
            "\n",
            "After mutating:\n",
            "tensor([[ 1, 20, 30,  4],\n",
            "        [ 5,  6,  7,  8]])\n",
            "tensor([20, 30,  4])\n",
            "tensor([ 2,  3, 40])\n",
            "False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1x/3jr_414d291dg6ht5ymfdb1c0000gn/T/ipykernel_39055/3860217186.py:18: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  print(a.storage().data_ptr() == c.storage().data_ptr())\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor, a slice, and a clone of a slice\n",
        "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "b = a[0, 1:]\n",
        "c = a[0, 1:].clone()\n",
        "print('Before mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "a[0, 1] = 20  # a[0, 1] and b[0] point to the same element\n",
        "b[1] = 30     # b[1] and a[0, 2] point to the same element\n",
        "c[2] = 40     # c is a clone, so it has its own data\n",
        "print('\\nAfter mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "print(a.storage().data_ptr() == c.storage().data_ptr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t5omyKwm9dB"
      },
      "source": [
        "Your turn: practice indexing tensors with slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKq2mswvqMmw"
      },
      "outputs": [],
      "source": [
        "# We will use this helper function to check your results\n",
        "def check(orig, actual, expected):\n",
        "  expected = torch.tensor(expected)\n",
        "  same_elements = (actual == expected).all().item() == 1\n",
        "  same_storage = (orig.storage().data_ptr() == actual.storage().data_ptr())\n",
        "  return same_elements and same_storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUnKm3Rjnr4t",
        "outputId": "a18b40a5-f2f6-4ec9-8c21-98a8e33a7813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b correct: True\n",
            "c correct: True\n",
            "d correct: True\n",
            "e correct: False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1x/3jr_414d291dg6ht5ymfdb1c0000gn/T/ipykernel_43812/3939471097.py:5: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  same_storage = (orig.storage().data_ptr() == actual.storage().data_ptr())\n"
          ]
        }
      ],
      "source": [
        "# Create the following rank 2 tensor of shape (3, 5)\n",
        "# [[ 1  2  3  4  5]\n",
        "#  [ 6  7  8  9 10]\n",
        "#  [11 12 13 14 15]]\n",
        "a = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 8, 10], [11, 12, 13, 14, 15]])\n",
        "\n",
        "b, c, d, e = None, None, None, None\n",
        "##############################################################################\n",
        "# TODO: Extract the last row of a, and store it in b; it should have rank 1. #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "b = a[-1,:]\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('b correct:', check(a, b, [11, 12, 13, 14, 15]))\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Extract the third col of a, and store it in c; it should have rank 2 #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "c = a[:,2:2]\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('c correct:', check(a, c, [[3], [8], [13]]))\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Use slicing to extract the first two rows and first three columns    #\n",
        "# from a; store the result into d.                                           #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "d = a[:2,:3]\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('d correct:', check(a, d, [[1, 2, 3], [6, 7, 8]]))\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Use slicing to extract a subtensor of a consisting of rows 0 and 2   #\n",
        "# and columns 1 and 4.                                                       #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "e = a[[0, 2]][:, [1, 4]]\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('e correct:', check(a, e, [[2, 5], [12, 15]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNjhLwb0xY2A"
      },
      "source": [
        "到目前为止，我们已经使用切片来**访问**子张量；我们也可以通过编写赋值表达式来使用切片**修改**子张量，其中左侧是切片表达式，右侧是常量或形状正确的张量："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFnky42Rx2I5",
        "outputId": "2117e0d4-4208-437c-e5dc-2838d84ce0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0, 0],\n",
            "        [0, 0, 0, 0]])\n",
            "tensor([[1, 1, 2, 3],\n",
            "        [1, 1, 4, 5]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.zeros(2, 4, dtype=torch.int64)\n",
        "a[:, :2] = 1\n",
        "a[:, 2:] = torch.tensor([[2, 3], [4, 5]])\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPVCQ5HszihV"
      },
      "source": [
        "Your turn: use slicing assignment to modify a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYR-jstVvUwE",
        "outputId": "c5554521-770c-4420-a3ed-65ebfaca7c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 2, 2, 2, 2],\n",
            "        [0, 1, 2, 2, 2, 2],\n",
            "        [3, 4, 3, 4, 5, 5],\n",
            "        [3, 4, 3, 4, 5, 5]])\n",
            "correct: True\n"
          ]
        }
      ],
      "source": [
        "x = torch.zeros(4, 6, dtype=torch.int64)\n",
        "##############################################################################\n",
        "# TODO: Use slicing to modify the tensor x so it has the following contents: #\n",
        "#     [[1, 0, 2, 2, 2, 2],                                                   #\n",
        "#      [0, 1, 2, 2, 2, 2],                                                   #\n",
        "#      [3, 4, 3, 4, 5, 5],                                                   #\n",
        "#      [3, 4, 3, 4, 5, 5]]                                                   #\n",
        "# This can be achieved using five slicing assignment operations.             #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "x[:2,:2] = torch.eye(2)\n",
        "x[:2,2:] = torch.ones(2,4) * 2\n",
        "x[2:,4:] = torch.ones(2,2)* 5\n",
        "x[2:,:4] = torch.tensor([[3,4,3,4],[3,4,3,4]])\n",
        "print(x)\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "\n",
        "expected = [\n",
        "    [1, 0, 2, 2, 2, 2],\n",
        "    [0, 1, 2, 2, 2, 2],\n",
        "    [3, 4, 3, 4, 5, 5],\n",
        "    [3, 4, 3, 4, 5, 5],\n",
        "]\n",
        "print('correct:', x.tolist() == expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y93rPhGveWw"
      },
      "source": [
        "### 整数张量索引"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlTyhjEN0AIE"
      },
      "source": [
        "当你使用切片对torch张量进行索引时，结果张量视图将始终是原始张量的一个子数组。这很强大，但可能有些限制。\n",
        "\n",
        "我们也可以使用 **索引数组** 来索引张量；这让我们在构建新张量时比使用切片有更多的灵活性。\n",
        "\n",
        "例如，我们可以使用索引数组重新排序张量的行或列："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXePPNkjM_SD",
        "outputId": "b8cb4017-c96a-472f-99d3-65a97579c6e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12]])\n",
            "\n",
            "Reordered rows:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 1,  2,  3,  4],\n",
            "        [ 9, 10, 11, 12],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 5,  6,  7,  8]])\n",
            "\n",
            "Reordered columns:\n",
            "tensor([[ 4,  3,  2,  1],\n",
            "        [ 8,  7,  6,  5],\n",
            "        [12, 11, 10,  9]])\n"
          ]
        }
      ],
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "# [[ 1  2  3  4]\n",
        "#  [ 5  6  7  8]\n",
        "#  [ 9 10 11 12]]\n",
        "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# 通过重新排列 a 的行创建一个形状为 (5, 4) 的新张量：\n",
        "\n",
        "# - 前两行与 a 的第一行相同\n",
        "# - 第三行与 a 的最后一行相同\n",
        "# - 第四和第五行与 a 的第二行相同\n",
        "idx = [0, 0, 2, 1, 1]  # index arrays can be Python lists of integers\n",
        "print('\\nReordered rows:')\n",
        "print(a[idx])\n",
        "\n",
        "# 通过反转a的列创建一个形状为(3, 4)的新张量。\n",
        "idx = torch.tensor([3, 2, 1, 0])  # Index arrays can be int64 torch tensors\n",
        "print('\\nReordered columns:')\n",
        "print(a[:, idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpIBR1bCQji6"
      },
      "source": [
        "More generally, given index arrays `idx0` and `idx1` with `N` elements each, `a[idx0, idx1]` is equivalent to:\n",
        "\n",
        "```\n",
        "torch.tensor([\n",
        "  a[idx0[0], idx1[0]],\n",
        "  a[idx0[1], idx1[1]],\n",
        "  ...,\n",
        "  a[idx0[N - 1], idx1[N - 1]]\n",
        "])\n",
        "```\n",
        "\n",
        "(A similar pattern extends to tensors with more than two dimensions)\n",
        "\n",
        "We can for example use this to get or set the diagonal of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocIR8R5ZSEaP",
        "outputId": "653ea2ce-0384-4a75-add0-729d52541ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "\n",
            "Get the diagonal:\n",
            "tensor([1, 5, 9])\n",
            "\n",
            "After setting the diagonal:\n",
            "tensor([[11,  2,  3],\n",
            "        [ 4, 22,  6],\n",
            "        [ 7,  8, 33]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "idx = [0, 1, 2]\n",
        "print('\\nGet the diagonal:')\n",
        "print(a[idx, idx])\n",
        "\n",
        "# Modify the diagonal\n",
        "a[idx, idx] = torch.tensor([11, 22, 33])\n",
        "print('\\nAfter setting the diagonal:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-cr-EqA0vfO"
      },
      "source": [
        "使用整数数组索引的一个有用技巧是从矩阵的每一行或每一列中选择或更改一个元素："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWA8E8iI0x17",
        "outputId": "807d6243-fb04-4bc5-8515-9e6a6ac03746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9],\n",
            "        [10, 11, 12]])\n",
            "\n",
            "Select one element from each row:\n",
            "tensor([ 2,  6,  8, 10])\n",
            "\n",
            "After modifying one element from each row:\n",
            "tensor([[ 1,  0,  3],\n",
            "        [ 4,  5,  0],\n",
            "        [ 7,  0,  9],\n",
            "        [ 0, 11, 12]])\n"
          ]
        }
      ],
      "source": [
        "# Create a new tensor from which we will select elements\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Take on element from each row of a:\n",
        "# from row 0, take element 1;\n",
        "# from row 1, take element 2;\n",
        "# from row 2, take element 1;\n",
        "# from row 3, take element 0\n",
        "idx0 = torch.arange(a.shape[0])  # Quick way to build [0, 1, 2, 3]\n",
        "idx1 = torch.tensor([1, 2, 1, 0])\n",
        "print('\\nSelect one element from each row:')\n",
        "print(a[idx0, idx1])\n",
        "\n",
        "# Now set each of those elements to zero\n",
        "a[idx0, idx1] = 0\n",
        "print('\\nAfter modifying one element from each row:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5_-WUmSVEoR"
      },
      "source": [
        "Your turn: practice with integer array indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFrrotu5VH2w",
        "outputId": "942852f5-e2d8-4966-da96-0dcc25278270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a:\n",
            "tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9],\n",
            "        [10, 11, 12]])\n",
            "\n",
            "Here is b:\n",
            "tensor([[ 1,  1,  3,  2],\n",
            "        [ 4,  4,  6,  5],\n",
            "        [ 7,  7,  9,  8],\n",
            "        [10, 10, 12, 11]])\n",
            "b correct: True\n",
            "\n",
            "Here is c:\n",
            "tensor([[10, 11, 12],\n",
            "        [ 7,  8,  9],\n",
            "        [ 4,  5,  6],\n",
            "        [ 1,  2,  3]])\n",
            "c correct: True\n",
            "\n",
            "Here is d:\n",
            "tensor([ 4,  2, 12])\n",
            "d correct: True\n"
          ]
        }
      ],
      "source": [
        "# Build a tensor of shape (4, 3):\n",
        "# [[ 1,  2,  3],\n",
        "#  [ 4,  5,  6],\n",
        "#  [ 7,  8,  9],\n",
        "#  [10, 11, 12]]\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "print('Here is a:')\n",
        "print(a)\n",
        "\n",
        "b, c, d = None, None, None\n",
        "##############################################################################\n",
        "# TODO: Use integer array indexing to create a tensor of shape (4, 4) where: #\n",
        "# - The first two columns are the same as the first column of a              #\n",
        "# - The next column is the same as the third column of a                     #\n",
        "# - The last column is the same as the second column of a                    #\n",
        "# Store the resulting tensor in b.                                           #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "idx2 = torch.tensor([0,0,2,1])\n",
        "b = a[:,idx2]\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('\\nHere is b:')\n",
        "print(b)\n",
        "expected = [[1, 1, 3, 2], [4, 4, 6, 5], [7, 7, 9, 8], [10, 10, 12, 11]]\n",
        "print('b correct:', b.tolist() == expected)\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Use integer array indexing to create a new tensor which is the same  #\n",
        "# as a, but has its rows reversed. Store the result in c.                    #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "idx3 = torch.tensor([3,2,1,0])\n",
        "c = a[idx3, :]\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('\\nHere is c:')\n",
        "print(c)\n",
        "expected = [[10, 11, 12], [7, 8, 9], [4, 5, 6], [1, 2, 3]]\n",
        "print('c correct:', c.tolist() == expected)\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Use integer array indexing to create a new tensor by selecting one   #\n",
        "# element from each column of a:                                             #\n",
        "# - From the first column, take the second element.                          #\n",
        "# - From the second column, take the first element.                          #\n",
        "# - From the third column, take the fourth element.                          #\n",
        "# Store the result in d.                                                     #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "idx4 = [0,1,2]\n",
        "idx5 = [1,0,3]\n",
        "d = a[idx5,idx4]\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('\\nHere is d:')\n",
        "print(d)\n",
        "expected = [4, 2, 12]\n",
        "print('d correct:', d.tolist() == expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGt8ZPb_vixw"
      },
      "source": [
        "### 布尔值索引"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CkQaRj01xmU"
      },
      "source": [
        "布尔张量索引允许您根据布尔掩码选择张量的任意元素。通常，这种类型的索引用于选择或修改满足某些条件的张量元素。\n",
        "\n",
        "在PyTorch中，我们使用dtype为`torch.uint8`的张量来保存布尔掩码；0表示假，1表示真。\n",
        "\n",
        "（PyTorch版本1.2.0引入了`torch.bool`类型的张量，用于布尔掩码，而不是使用`torch.uint8`。然而在本课程中，我们使用的是PyTorch 1.1.0）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29Zf7rb82Dkd",
        "outputId": "2c599f74-228f-4188-a635-44a080329680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "\n",
            "Mask tensor:\n",
            "tensor([[False, False],\n",
            "        [False,  True],\n",
            "        [ True,  True]])\n",
            "\n",
            "Selecting elements with the mask:\n",
            "tensor([4, 5, 6])\n",
            "\n",
            "After modifying with a mask:\n",
            "tensor([[0, 0],\n",
            "        [0, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([[1,2], [3, 4], [5, 6]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# 找出 a 中大于 3 的元素。mask 与 a 形状相同，其中每个 mask 元素指示 a 的相应元素是否大于 3。\n",
        "mask = (a > 3)\n",
        "print('\\nMask tensor:')\n",
        "print(mask)\n",
        "\n",
        "# We can use the mask to construct a rank-1 tensor containing the elements of a\n",
        "# that are selected by the mask\n",
        "print('\\nSelecting elements with the mask:')\n",
        "print(a[mask])\n",
        "\n",
        "# We can also use boolean masks to modify tensors; for example this sets all\n",
        "# elements <= 3 to zero:\n",
        "a[a <= 3] = 0\n",
        "print('\\nAfter modifying with a mask:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtSmmMGodrTX"
      },
      "source": [
        "Your turn: practice with boolean masks by implementing the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hkeYXN9d5xh",
        "outputId": "51d61d21-0701-4eb6-ae7d-37925e01b73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_negative seems to be correct!\n"
          ]
        }
      ],
      "source": [
        "def num_negative(x):\n",
        "  \"\"\"\n",
        "  Return the number of negative values in the tensor x\n",
        "\n",
        "  Inputs:\n",
        "  - x: A tensor of any shape\n",
        "\n",
        "  Returns:\n",
        "  - num_neg: Number of negative values in x\n",
        "  \"\"\"\n",
        "  num_neg = 0\n",
        "  ##############################################################################\n",
        "  # TODO: Use boolean masks to count the number of negative elements in x.     #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  num_neg = torch.sum(x < 0).item()\n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return num_neg\n",
        "\n",
        "# Make a few test cases\n",
        "torch.manual_seed(598)\n",
        "x0 = torch.tensor([[-1, -1, 0], [0, 1, 2], [3, 4, 5]])\n",
        "x1 = torch.tensor([0, 1, 2, 3])\n",
        "x2 = torch.randn(100, 100)\n",
        "assert num_negative(x0) == 2\n",
        "assert num_negative(x1) == 0\n",
        "assert num_negative(x2) == 4984\n",
        "print('num_negative seems to be correct!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q68ZApgH31W9"
      },
      "source": [
        "现在实现一个函数，从一个Python整数列表创建一个 **独热向量(one-hot vector)** 矩阵。\n",
        "\n",
        "一个整数 $n$ 的独热向量是在其第 $n$ 个位置为1，其他位置为0的向量。独热向量通常用于在机器学习模型中表示分类变量。\n",
        "\n",
        "例如，给定一个整数列表`[1, 4, 3, 2]`，你的函数应该生成张量：\n",
        "\n",
        "```\n",
        "[[0 1 0 0 0],\n",
        " [0 0 0 0 1],\n",
        " [0 0 0 1 0],\n",
        " [0 0 1 0 0]]\n",
        "```\n",
        "\n",
        "Here the first row corresponds to the first element of the list: it has a one at index 1, and zeros at all other indices. The second row corresponds to the second element of the list: it has a one at index 4, and zeros at all other indices. The other rows follow the same pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZfBmBFo4HRx"
      },
      "outputs": [],
      "source": [
        "def make_one_hot(x):\n",
        "  \"\"\"\n",
        "  Construct a tensor of one-hot-vectors from a list of Python integers.\n",
        "\n",
        "  Input:\n",
        "  - x: A list of N ints\n",
        "\n",
        "  Returns:\n",
        "  - y: A tensor of shape (N, C) where C = 1 + max(x) is one more than the max\n",
        "       value in x. The nth row of y is a one-hot-vector representation of x[n];\n",
        "       In other words, if x[n] = c then y[n, c] = 1; all other elements of y are\n",
        "       zeros.\n",
        "  \"\"\"\n",
        "  y = None\n",
        "  ##############################################################################\n",
        "  # TODO: Complete the implementation of this function.                        #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  rows = len(x)\n",
        "  columns = max(x) + 1\n",
        "  y = torch.zeros(rows,columns)\n",
        "  for i in range(rows):\n",
        "    y[i,x[i]] = 1\n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKTWTL-L9mx5"
      },
      "source": [
        "Now check your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaT1kuQ37Rsq",
        "outputId": "b7f25865-c626-4c24-dd50-14c47ef2ea8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is y0:\n",
            "tensor([[0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0.]])\n",
            "\n",
            "Here is y1:\n",
            "tensor([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0.]])\n",
            "all checks pass!\n"
          ]
        }
      ],
      "source": [
        "def check_one_hot(x, y):\n",
        "  C = y.shape[1]\n",
        "  for i, n in enumerate(x):\n",
        "    if n >= C: return False\n",
        "    for j in range(C):\n",
        "      expected = 1.0 if j == n else 0.0\n",
        "      if y[i, j].item() != expected: return False\n",
        "  return True\n",
        "\n",
        "x0 = [1, 4, 3, 2]\n",
        "y0 = make_one_hot(x0)\n",
        "print('Here is y0:')\n",
        "print(y0)\n",
        "assert check_one_hot(x0, y0), 'y0 is wrong'\n",
        "\n",
        "x1 = [1, 3, 5, 7, 6, 2]\n",
        "y1 = make_one_hot(x1)\n",
        "print('\\nHere is y1:')\n",
        "print(y1)\n",
        "assert check_one_hot(x1, y1), 'y1 is wrong'\n",
        "\n",
        "print('all checks pass!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-xqELwyqpN"
      },
      "source": [
        "## Reshaping operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql9_eXuU4OG8"
      },
      "source": [
        "### View"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfPb_2BY0HKw"
      },
      "source": [
        "PyTorch提供了多种方法来操作张量的形状。 最简单的是[`.view()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.view): 这将返回一个与其输入具有相同元素数量但形状不同的新张量。\n",
        "\n",
        "我们可以使用 `.view()` 将矩阵展平成向量，并将秩为1的向量转换为秩为2的行或列矩阵："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw-M7C_61FZK",
        "outputId": "1f7fe3b6-36a3-467d-f3e2-c952877f844d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "shape: torch.Size([2, 4])\n",
            "\n",
            "Flattened tensor:\n",
            "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
            "shape: torch.Size([8])\n",
            "\n",
            "Row vector:\n",
            "tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
            "shape: torch.Size([1, 8])\n",
            "\n",
            "Column vector:\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5],\n",
            "        [6],\n",
            "        [7],\n",
            "        [8]])\n",
            "shape: torch.Size([8, 1])\n",
            "\n",
            "Rank 3 tensor:\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "shape: torch.Size([2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "x0 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "print('Original tensor:')\n",
        "print(x0)\n",
        "print('shape:', x0.shape)\n",
        "\n",
        "# Flatten x0 into a rank 1 vector of shape (8,)\n",
        "x1 = x0.view(8)\n",
        "print('\\nFlattened tensor:')\n",
        "print(x1)\n",
        "print('shape:', x1.shape)\n",
        "\n",
        "# Convert x1 to a rank 2 \"row vector\" of shape (1, 8)\n",
        "x2 = x1.view(1, 8)\n",
        "print('\\nRow vector:')\n",
        "print(x2)\n",
        "print('shape:', x2.shape)\n",
        "\n",
        "# Convert x1 to a rank 2 \"column vector\" of shape (8, 1)\n",
        "x3 = x1.view(8, 1)\n",
        "print('\\nColumn vector:')\n",
        "print(x3)\n",
        "print('shape:', x3.shape)\n",
        "\n",
        "# Convert x1 to a rank 3 tensor of shape (2, 2, 2):\n",
        "x4 = x1.view(2, 2, 2)\n",
        "print('\\nRank 3 tensor:')\n",
        "print(x4)\n",
        "print('shape:', x4.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHsZ8BPF2PEq"
      },
      "source": [
        "为了方便，调用 `.view()` 时可以包含一个 -1 参数；这会在该维度上放置足够的元素，使输出与输入具有相同的形状。这使得以一种与张量形状无关的方式编写一些重塑操作变得容易："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNWu-R_J2qFY",
        "outputId": "99e2174d-7d0d-4890-a18c-ab6d69f14ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x0:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "x0_flat:\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "x0_row:\n",
            "tensor([[1, 2, 3, 4, 5, 6]])\n",
            "\n",
            "x1:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "x1_flat:\n",
            "tensor([1, 2, 3, 4])\n",
            "x1_row:\n",
            "tensor([[1, 2, 3, 4]])\n"
          ]
        }
      ],
      "source": [
        "# We can reuse these functions for tensors of different shapes\n",
        "def flatten(x):\n",
        "  return x.view(-1)\n",
        "\n",
        "def make_row_vec(x):\n",
        "  return x.view(1, -1)\n",
        "\n",
        "x0 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "x0_flat = flatten(x0)\n",
        "x0_row = make_row_vec(x0)\n",
        "print('x0:')\n",
        "print(x0)\n",
        "print('x0_flat:')\n",
        "print(x0_flat)\n",
        "print('x0_row:')\n",
        "print(x0_row)\n",
        "\n",
        "x1 = torch.tensor([[1, 2], [3, 4]])\n",
        "x1_flat = flatten(x1)\n",
        "x1_row = make_row_vec(x1)\n",
        "print('\\nx1:')\n",
        "print(x1)\n",
        "print('x1_flat:')\n",
        "print(x1_flat)\n",
        "print('x1_row:')\n",
        "print(x1_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK-ZB5aB2NPq"
      },
      "source": [
        "顾名思义，`.view() `返回的张量与输入的张量共享相同的数据，因此对其中一个张量的更改会影响另一个张量，反之亦然："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebT99rUo2McN",
        "outputId": "3ec0d29b-3dbd-4614-9c9c-34578ee49113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x before modifying:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "x_flat before modifying:\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "\n",
            "x after modifying:\n",
            "tensor([[10, 20,  3],\n",
            "        [ 4,  5,  6]])\n",
            "x_flat after modifying:\n",
            "tensor([10, 20,  3,  4,  5,  6])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "x_flat = x.view(-1)\n",
        "print('x before modifying:')\n",
        "print(x)\n",
        "print('x_flat before modifying:')\n",
        "print(x_flat)\n",
        "\n",
        "x[0, 0] = 10   # x[0, 0] and x_flat[0] point to the same data\n",
        "x_flat[1] = 20 # x_flat[1] and x[0, 1] point to the same data\n",
        "\n",
        "print('\\nx after modifying:')\n",
        "print(x)\n",
        "print('x_flat after modifying:')\n",
        "print(x_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z150qBob4Wkz"
      },
      "source": [
        "### Swapping axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMDxbyBys78"
      },
      "source": [
        "另一种常见的重塑操作是转置矩阵。如果你尝试使用 `.view()` 转置矩阵，可能会感到惊讶：`view()` 函数以行优先顺序获取元素，因此**你不能使用 `.view()` 转置矩阵**。\n",
        "\n",
        "通常，你应该只使用 `.view()` 来为张量添加新维度，或折叠张量的相邻维度。\n",
        "\n",
        "对于其他类型的重塑操作，通常需要使用可以交换张量轴的函数。最简单的此类函数是`.t()`，专门用于转置矩阵。它是`torch`模块中的一个[函数](https://pytorch.org/docs/1.1.0/torch.html#torch.t),以及一个 [tensor 实例的方法](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.t):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_B4NuX6zQm-",
        "outputId": "b98ab01e-2224-42e5-ec9f-398efaa46977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original matrix:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Transposing with view DOES NOT WORK!\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "\n",
            "Transposed matrix:\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print('Original matrix:')\n",
        "print(x)\n",
        "print('\\nTransposing with view DOES NOT WORK!')\n",
        "print(x.view(3, 2))\n",
        "print('\\nTransposed matrix:')\n",
        "print(torch.t(x))\n",
        "print(x.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN93xo98zn0v"
      },
      "source": [
        "对于高于二维的张量，可使用函数 [`torch.transpose`](https://pytorch.org/docs/1.1.0/torch.html#torch.transpose) 交换任意维度, 或者 [`.permute`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.permute)方法排列任意维数:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgN7YB8YzzkA",
        "outputId": "2e214db6-ca74-42ee-fb16-b399bb8aed21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[[ 1,  2,  3,  4],\n",
            "         [ 5,  6,  7,  8],\n",
            "         [ 9, 10, 11, 12]],\n",
            "\n",
            "        [[13, 14, 15, 16],\n",
            "         [17, 18, 19, 20],\n",
            "         [21, 22, 23, 24]]])\n",
            "shape: torch.Size([2, 3, 4])\n",
            "\n",
            "Swap axes 1 and 2:\n",
            "tensor([[[ 1,  5,  9],\n",
            "         [ 2,  6, 10],\n",
            "         [ 3,  7, 11],\n",
            "         [ 4,  8, 12]],\n",
            "\n",
            "        [[13, 17, 21],\n",
            "         [14, 18, 22],\n",
            "         [15, 19, 23],\n",
            "         [16, 20, 24]]])\n",
            "torch.Size([2, 4, 3])\n",
            "\n",
            "Permute axes\n",
            "tensor([[[ 1, 13],\n",
            "         [ 2, 14],\n",
            "         [ 3, 15],\n",
            "         [ 4, 16]],\n",
            "\n",
            "        [[ 5, 17],\n",
            "         [ 6, 18],\n",
            "         [ 7, 19],\n",
            "         [ 8, 20]],\n",
            "\n",
            "        [[ 9, 21],\n",
            "         [10, 22],\n",
            "         [11, 23],\n",
            "         [12, 24]]])\n",
            "shape: torch.Size([3, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor of shape (2, 3, 4)\n",
        "x0 = torch.tensor([\n",
        "     [[1,  2,  3,  4],\n",
        "      [5,  6,  7,  8],\n",
        "      [9, 10, 11, 12]],\n",
        "     [[13, 14, 15, 16],\n",
        "      [17, 18, 19, 20],\n",
        "      [21, 22, 23, 24]]])\n",
        "print('Original tensor:')\n",
        "print(x0)\n",
        "print('shape:', x0.shape)\n",
        "\n",
        "# Swap axes 1 and 2; shape is (2, 4, 3)\n",
        "x1 = x0.transpose(1, 2)\n",
        "print('\\nSwap axes 1 and 2:')\n",
        "print(x1)\n",
        "print(x1.shape)\n",
        "\n",
        "# Permute axes; the argument (1, 2, 0) means:\n",
        "# - Make the old dimension 1 appear at dimension 0;\n",
        "# - Make the old dimension 2 appear at dimension 1;\n",
        "# - Make the old dimension 0 appear at dimension 2\n",
        "# This results in a tensor of shape (3, 4, 2)\n",
        "x2 = x0.permute(1, 2, 0)\n",
        "print('\\nPermute axes')\n",
        "print(x2)\n",
        "print('shape:', x2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4SJCVbf-bZ0"
      },
      "source": [
        "### Contiguous tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubOOujO_-pQT"
      },
      "source": [
        "有些重塑操作组合会失败，并伴有隐性错误。具体原因与实现张量和张量视图的方式有关，超出了本作业的范围。 However if you're curious, [this blog post by Edward Yang](http://blog.ezyang.com/2019/05/pytorch-internals/) gives a clear explanation of the problem.\n",
        "\n",
        "What you need to know is that you can typically overcome these sorts of errors by either by calling [`.contiguous()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.contiguous) before `.view()`, or by using [`.reshape()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.reshape) instead of `.view()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGC6NERq_CT9"
      },
      "outputs": [],
      "source": [
        "x0 = torch.randn(2, 3, 4)\n",
        "\n",
        "try:\n",
        "  # This sequence of reshape operations will crash\n",
        "  x1 = x0.transpose(1, 2).view(8, 3)\n",
        "except RuntimeError as e:\n",
        "  print(type(e), e)\n",
        "\n",
        "# We can solve the problem using either .contiguous() or .reshape()\n",
        "x1 = x0.transpose(1, 2).contiguous().view(8, 3)\n",
        "x2 = x0.transpose(1, 2).reshape(8, 3)\n",
        "print('x1 shape: ', x1.shape)\n",
        "print('x2 shape: ', x2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJiiBxNE-X8g"
      },
      "source": [
        "### Your turn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOVzHiX-86Ew"
      },
      "source": [
        "Given the 1-dimensional input tensor `x0` containing the numbers 0 through 23 in order, apply a sequence of reshaping operations to `x0` to create the following tensor:\n",
        "\n",
        "```\n",
        "x1 = tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n",
        "             [ 4,  5,  6,  7, 16, 17, 18, 19],\n",
        "             [ 8,  9, 10, 11, 20, 21, 22, 23]])\n",
        "```\n",
        "\n",
        "Hint: You will need to create an intermediate tensor of rank 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI59iIdS8Hms",
        "outputId": "1e1721d3-2107-4a20-f795-a2c40af8a483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is x0:\n",
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23])\n",
            "\n",
            "Here is x1:\n",
            "tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n",
            "        [ 4,  5,  6,  7, 16, 17, 18, 19],\n",
            "        [ 8,  9, 10, 11, 20, 21, 22, 23]])\n",
            "Correct: True\n"
          ]
        }
      ],
      "source": [
        "x0 = torch.arange(24)\n",
        "print('Here is x0:')\n",
        "print(x0)\n",
        "\n",
        "x1 = None\n",
        "##############################################################################\n",
        "# TODO: Use reshape operations to create x1 from x0                          #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "xx = x0.reshape(2, 3, 4)\n",
        "xxx = xx.permute(1,0,2)\n",
        "x1 = xxx.reshape(3,8)\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('\\nHere is x1:')\n",
        "print(x1)\n",
        "\n",
        "expected = [\n",
        "    [0, 1,  2,  3, 12, 13, 14, 15],\n",
        "    [4, 5,  6,  7, 16, 17, 18, 19],\n",
        "    [8, 9, 10, 11, 20, 21, 22, 23]]\n",
        "print('Correct:', x1.tolist() == expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgcdvD1evxTQ"
      },
      "source": [
        "## 张量操作"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BCVlPHZ4_Qz"
      },
      "source": [
        "### Elementwise operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2wbN18E5CKI"
      },
      "source": [
        "基本数学函数对张量进行逐元素操作，可以作为运算符重载、`torch`模块中的函数以及torch对象的实例方法使用；所有这些方法产生的结果相同："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrMkbk535KRZ",
        "outputId": "bfe5e7c4-be6a-44de-d27f-bc8024cb9566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elementwise sum:\n",
            "tensor([[ 6.,  8., 10., 12.]])\n",
            "tensor([[ 6.,  8., 10., 12.]])\n",
            "tensor([[ 6.,  8., 10., 12.]])\n",
            "\n",
            "Elementwise difference:\n",
            "tensor([[-4., -4., -4., -4.]])\n",
            "tensor([[-4., -4., -4., -4.]])\n",
            "tensor([[-4., -4., -4., -4.]])\n",
            "\n",
            "Elementwise product:\n",
            "tensor([[ 5., 12., 21., 32.]])\n",
            "tensor([[ 5., 12., 21., 32.]])\n",
            "tensor([[ 5., 12., 21., 32.]])\n",
            "\n",
            "Elementwise division\n",
            "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
            "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
            "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
            "\n",
            "Elementwise power\n",
            "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
            "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
            "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
        "y = torch.tensor([[5, 6, 7, 8]], dtype=torch.float32)\n",
        "\n",
        "# Elementwise sum; all give the same result\n",
        "print('Elementwise sum:')\n",
        "print(x + y)\n",
        "print(torch.add(x, y))\n",
        "print(x.add(y))\n",
        "\n",
        "# Elementwise difference\n",
        "print('\\nElementwise difference:')\n",
        "print(x - y)\n",
        "print(torch.sub(x, y))\n",
        "print(x.sub(y))\n",
        "\n",
        "# Elementwise product\n",
        "print('\\nElementwise product:')\n",
        "print(x * y)\n",
        "print(torch.mul(x, y))\n",
        "print(x.mul(y))\n",
        "\n",
        "# Elementwise division\n",
        "print('\\nElementwise division')\n",
        "print(x / y)\n",
        "print(torch.div(x, y))\n",
        "print(x.div(y))\n",
        "\n",
        "# Elementwise power\n",
        "print('\\nElementwise power')\n",
        "print(x ** y)\n",
        "print(torch.pow(x, y))\n",
        "print(x.pow(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6WwPJMYlYvN"
      },
      "source": [
        "Torch 还提供了许多标准数学函数；这些函数既可以作为 `torch` 模块中的函数使用，也可以作为张量上的实例方法使用：\n",
        "\n",
        "You can find a full list of all available mathematical functions [in the documentation](https://pytorch.org/docs/stable/torch.html#pointwise-ops); many functions in the `torch` module have corresponding instance methods [on tensor objects](https://pytorch.org/docs/stable/tensors.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s87mjsnG58vR",
        "outputId": "bcc4fa00-5b01-4f72-d12a-a229a0bf89c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Square root:\n",
            "tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n",
            "tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n",
            "\n",
            "Trig functions:\n",
            "tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n",
            "tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n",
            "tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n",
            "tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
        "\n",
        "print('Square root:')\n",
        "print(torch.sqrt(x))\n",
        "print(x.sqrt())\n",
        "\n",
        "print('\\nTrig functions:')\n",
        "print(torch.sin(x))\n",
        "print(x.sin())\n",
        "print(torch.cos(x))\n",
        "print(x.cos())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDyH9USAuyZ-"
      },
      "source": [
        "### Reduction operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbHP9SpZHoMO"
      },
      "source": [
        "到目前为止，我们已经看到了对张量进行逐元素操作的基本算术运算。有时我们可能希望执行对张量的部分或全部进行聚合的操作，例如求和；这些称为 **归约(reduction)** 操作。\n",
        "\n",
        "与上述逐元素操作类似，大多数归约操作既可以作为 `torch` 模块中的函数使用，也可以作为 `tensor` 对象上的实例方法使用。\n",
        "\n",
        "最简单的归约操作是求和。我们可以使用 `.sum()` 函数来对整个张量进行归约，或者使用 `dim` 参数仅沿张量的一个维度进行归约："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlmsYJWUE2r3",
        "outputId": "c06d52c4-e10d-4d33-98ac-e1d43ad799e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "Sum over entire tensor:\n",
            "tensor(21.)\n",
            "tensor(21.)\n",
            "\n",
            "Sum of each row:\n",
            "tensor([5., 7., 9.])\n",
            "tensor([5., 7., 9.])\n",
            "\n",
            "Sum of each column:\n",
            "tensor([ 6., 15.])\n",
            "tensor([ 6., 15.])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]], dtype=torch.float32)\n",
        "print('Original tensor:')\n",
        "print(x)\n",
        "\n",
        "print('\\nSum over entire tensor:')\n",
        "print(torch.sum(x))\n",
        "print(x.sum())\n",
        "\n",
        "# We can sum over each row:\n",
        "print('\\nSum of each row:')\n",
        "print(torch.sum(x, dim=0))\n",
        "print(x.sum(dim=0))\n",
        "\n",
        "# Sum over each column:\n",
        "print('\\nSum of each column:')\n",
        "print(torch.sum(x, dim=1))\n",
        "print(x.sum(dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzKio_3Quz5a"
      },
      "source": [
        "Other useful reduction operations include [`mean`](https://pytorch.org/docs/stable/torch.html#torch.mean), [`min`](https://pytorch.org/docs/stable/torch.html#torch.min), and [`max`](https://pytorch.org/docs/stable/torch.html#torch.max). You can find a full list of all available reduction operations [in the documentation](https://pytorch.org/docs/stable/torch.html#reduction-ops).\n",
        "\n",
        "某些还原操作会返回多个值，例如 `min` 既返回指定维度上的最小值，也返回最小值所在的索引："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFD7aT54H4ik",
        "outputId": "7b2fe498-8a2b-44a4-f78b-7ba71d9f1856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor:\n",
            "tensor([[2., 4., 3., 5.],\n",
            "        [3., 3., 5., 2.]]) torch.Size([2, 4])\n",
            "\n",
            "Overall minimum:  tensor(2.)\n",
            "\n",
            "Minimum along each column:\n",
            "values: tensor([2., 3., 3., 2.])\n",
            "idxs: tensor([0, 1, 0, 1])\n",
            "\n",
            "Minimum along each row:\n",
            "values: tensor([2., 2.])\n",
            "idxs: tensor([0, 3])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[2, 4, 3, 5], [3, 3, 5, 2]], dtype=torch.float32)\n",
        "print('Original tensor:')\n",
        "print(x, x.shape)\n",
        "\n",
        "# Finding the overall minimum only returns a single value\n",
        "print('\\nOverall minimum: ', x.min())\n",
        "\n",
        "# Compute the minimum along each column; we get both the value and location:\n",
        "# 第一列的最小值是2，出现在索引0处；\n",
        "# 第二列的最小值是3，出现在索引1处；等等。\n",
        "col_min_vals, col_min_idxs = x.min(dim=0)\n",
        "print('\\nMinimum along each column:')\n",
        "print('values:', col_min_vals)\n",
        "print('idxs:', col_min_idxs)\n",
        "\n",
        "# Compute the minimum along each row; we get both the value and the minimum\n",
        "row_min_vals, row_min_idxs = x.min(dim=1)\n",
        "print('\\nMinimum along each row:')\n",
        "print('values:', row_min_vals)\n",
        "print('idxs:', row_min_idxs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFwYRESoFr4t"
      },
      "source": [
        "归约操作会 *减少* 张量的秩：执行归约的维度将从输出的形状中移除。如果在归约操作中传递 `keepdim=True`，则指定的维度不会被移除；输出张量在该维度上将具有1的形状。\n",
        "\n",
        "当处理多维张量时，考虑行和列可能会变得混乱；相反，考虑每个操作将产生的形状更有用。例如："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjcAveyJFqm7",
        "outputId": "65348a77-46d7-4b74-c94e-abfdd46a4cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 10, 3, 64, 64])\n",
            "torch.Size([128, 3, 64, 64])\n",
            "torch.Size([128, 3, 64])\n",
            "torch.Size([128, 1, 64])\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor of shape (128, 10, 3, 64, 64)\n",
        "x = torch.randn(128, 10, 3, 64, 64)\n",
        "print(x.shape)\n",
        "\n",
        "# Take the mean over dimension 1; shape is now (128, 3, 64, 64)\n",
        "x = x.mean(dim=1)\n",
        "print(x.shape)\n",
        "\n",
        "# Take the sum over dimension 2; shape is now (128, 3, 64)\n",
        "x = x.sum(dim=2)\n",
        "print(x.shape)\n",
        "\n",
        "# Take the mean over dimension 1, but keep the dimension from being eliminated\n",
        "# by passing keepdim=True; shape is now (128, 1, 64)\n",
        "x = x.mean(dim=1, keepdim=True)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXMp4tcM0Q_E"
      },
      "source": [
        "Your turn: use reduction and indexing operations to implement a function that sets the minimum value along each row of a tensor to zero.\n",
        "\n",
        "Hint: [`torch.argmin`](https://pytorch.org/docs/stable/torch.html#torch.argmin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzD-QVTR0bRu"
      },
      "outputs": [],
      "source": [
        "def zero_row_min(x):\n",
        "  \"\"\"\n",
        "  Return a copy of x, where the minimum value along each row has been set to 0.\n",
        "\n",
        "  For example, if x is:\n",
        "  x = torch.tensor([[\n",
        "        [10, 20, 30],\n",
        "        [ 2,  5,  1]\n",
        "      ]])\n",
        "\n",
        "  Then y = zero_row_min(x) should be:\n",
        "  torch.tensor([\n",
        "    [0, 20, 30],\n",
        "    [2,  5,  0]\n",
        "  ])\n",
        "\n",
        "  Inputs:\n",
        "  - x: Tensor of rank 2 with shape (N, M)\n",
        "\n",
        "  Returns:\n",
        "  - y: Tensor of rank 2 that is a copy of x, except the minimum value along each\n",
        "       row is replaced with 0.\n",
        "  \"\"\"\n",
        "  y = x.clone()\n",
        "  ##############################################################################\n",
        "  # TODO: Complete the implementation of this function.                        #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  ind = torch.argmin(y,dim=1)\n",
        "  for i in range(ind.shape[0]):\n",
        "    y[i,ind[i]] = 0\n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8a8ZEzJ2xv_"
      },
      "source": [
        "Now test your implementation with a few small test cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaJzt-Y62blF",
        "outputId": "3ddbb2ac-ef45-4438-cf38-c24753840ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is x0:\n",
            "tensor([[10, 20, 30],\n",
            "        [ 2,  5,  1]])\n",
            "Here is y0:\n",
            "tensor([[ 0, 20, 30],\n",
            "        [ 2,  5,  0]])\n",
            "\n",
            "Here is x1:\n",
            "tensor([[ 2,  5, 10, -1],\n",
            "        [ 1,  3,  2,  4],\n",
            "        [ 5,  6,  2, 10]])\n",
            "Here is y1:\n",
            "tensor([[ 2,  5, 10,  0],\n",
            "        [ 0,  3,  2,  4],\n",
            "        [ 5,  6,  0, 10]])\n",
            "\n",
            "Simple tests pass!\n"
          ]
        }
      ],
      "source": [
        "x0 = torch.tensor([[10, 20, 30], [2, 5, 1]])\n",
        "print('Here is x0:')\n",
        "print(x0)\n",
        "y0 = zero_row_min(x0)\n",
        "print('Here is y0:')\n",
        "print(y0)\n",
        "assert y0.tolist() == [[0, 20, 30], [2, 5, 0]]\n",
        "\n",
        "x1 = torch.tensor([[2, 5, 10, -1], [1, 3, 2, 4], [5, 6, 2, 10]])\n",
        "print('\\nHere is x1:')\n",
        "print(x1)\n",
        "y1 = zero_row_min(x1)\n",
        "print('Here is y1:')\n",
        "print(y1)\n",
        "assert y1.tolist() == [[2, 5, 10, 0], [0, 3, 2, 4], [5, 6, 0, 10]]\n",
        "\n",
        "print('\\nSimple tests pass!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRyLyXU2u29N"
      },
      "source": [
        "### 矩阵操作"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DwjbapG6MM_"
      },
      "source": [
        "请注意，与 MATLAB 不同，* 是对应元素相乘，而不是矩阵乘法。PyTorch 提供了许多线性代数函数来计算不同类型的向量和矩阵乘积。最常用的是：\n",
        "\n",
        "- [`torch.dot`](https://pytorch.org/docs/1.1.0/torch.html#blas-and-lapack-operations): Computes inner product of vectors\n",
        "- [`torch.mm`](https://pytorch.org/docs/1.1.0/torch.html#torch.mm): Computes matrix-matrix products\n",
        "- [`torch.mv`](https://pytorch.org/docs/1.1.0/torch.html#torch.mv): Computes matrix-vector products\n",
        "- [`torch.addmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.addmm) / [`torch.addmv`](https://pytorch.org/docs/1.1.0/torch.html#torch.addmv): Computes matrix-matrix and matrix-vector multiplications plus a bias\n",
        "- [`torch.bmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.addmv) / [`torch.baddmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.baddbmm): Batched versions of `torch.mm` and `torch.addmm`, respectively\n",
        "- [`torch.matmul`](https://pytorch.org/docs/1.1.0/torch.html#torch.matmul): General matrix product that performs different operations depending on the rank of the inputs; this is similar to `np.dot` in numpy.\n",
        "\n",
        "You can find a full list of the available linear algebra operators [in the documentation](https://pytorch.org/docs/1.1.0/torch.html#blas-and-lapack-operations).\n",
        "\n",
        "以下是使用 `torch.dot` 计算内积的示例。与我们见过的其他数学运算符一样，大多数线性代数运算符既可以作为 `torch` 模块中的函数使用，也可以作为张量的实例方法使用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRUYW2as6ZCh",
        "outputId": "4475b0bf-9418-4e34-c611-8e57872742c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dot products:\n",
            "tensor(219.)\n",
            "tensor(219.)\n",
            "1D tensors expected, but got 2D and 2D tensors\n",
            "\n",
            "Matrix-matrix product:\n",
            "tensor([[19., 22.],\n",
            "        [43., 50.]])\n",
            "tensor([[19., 22.],\n",
            "        [43., 50.]])\n"
          ]
        }
      ],
      "source": [
        "v = torch.tensor([9,10], dtype=torch.float32)\n",
        "w = torch.tensor([11, 12], dtype=torch.float32)\n",
        "\n",
        "# Inner product of vectors\n",
        "print('Dot products:')\n",
        "print(torch.dot(v, w))\n",
        "print(v.dot(w))\n",
        "\n",
        "# dot only works for vectors -- it will give an error for tensors of rank > 1 只有一维张量才能用 dot 方法\n",
        "x = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
        "y = torch.tensor([[5,6],[7,8]], dtype=torch.float32)\n",
        "try:\n",
        "  print(x.dot(y))\n",
        "except RuntimeError as e:\n",
        "  print(e)\n",
        "\n",
        "# Instead we use mm for matrix-matrix products: 二维张量就要用 mm 方法\n",
        "print('\\nMatrix-matrix product:')\n",
        "print(torch.mm(x, y))\n",
        "print(x.mm(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRxK34KdHm3"
      },
      "source": [
        "PyTorch 提供了多种线性代数运算符，通常有不止一种方法来计算某个操作。例如，要计算矩阵向量乘积，我们可以使用 `torch.mv`；我们可以将向量重塑为秩为2的张量并使用 `torch.mm`；或者我们可以使用 `torch.matmul`。所有方法都会得到相同的结果，但输出的秩可能不同："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqEzcnHkdRYA",
        "outputId": "dff4ee69-a6e6-48f1-8786-714f103b05bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is x (rank 2):\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "\n",
            "Here is v (rank 1):\n",
            "tensor([ 9., 10.])\n",
            "\n",
            "Matrix-vector product with torch.mv (rank 1 output)\n",
            "tensor([29., 67.])\n",
            "tensor([29., 67.])\n",
            "\n",
            "Matrix-vector product with torch.mm (rank 2 output)\n",
            "tensor([[29.],\n",
            "        [67.]])\n",
            "tensor([[29.],\n",
            "        [67.]])\n",
            "\n",
            "Matrix-vector product with torch.matmul (rank 1 output)\n",
            "tensor([29., 67.])\n",
            "tensor([29., 67.])\n"
          ]
        }
      ],
      "source": [
        "print('Here is x (rank 2):')\n",
        "print(x)\n",
        "print('\\nHere is v (rank 1):')\n",
        "print(v)\n",
        "\n",
        "# Matrix-vector multiply with torch.mv produces a rank-1 output\n",
        "print('\\nMatrix-vector product with torch.mv (rank 1 output)')\n",
        "print(torch.mv(x, v))\n",
        "print(x.mv(v))\n",
        "\n",
        "# We can reshape the vector to have rank 2 and use torch.mm to perform\n",
        "# matrix-vector products, but the result will have rank 2\n",
        "print('\\nMatrix-vector product with torch.mm (rank 2 output)')\n",
        "print(torch.mm(x, v.view(2, 1)))\n",
        "print(x.mm(v.view(2, 1)))\n",
        "\n",
        "print('\\nMatrix-vector product with torch.matmul (rank 1 output)')\n",
        "print(torch.matmul(x, v))\n",
        "print(x.matmul(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eqQJ5IUjtNT"
      },
      "source": [
        "Your turn: use [`torch.bmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.bmm) to perform a batched matrix multiply.\n",
        "\n",
        "对存储在 batch1 和 batch2 中的矩阵执行批量矩阵乘法。\n",
        "\n",
        "batch1 和 batch2 必须是每个包含相同数量矩阵的三维张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKLK85m4jyLe",
        "outputId": "195bc1cf-9240-4ede-ec66-f5d36f763ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the result of batched matrix multiply with a loop:\n",
            "tensor([[[0.9347, 0.7368, 1.1844, 1.1910],\n",
            "         [1.1043, 0.5611, 0.8835, 0.9876]],\n",
            "\n",
            "        [[0.7110, 0.8649, 1.5978, 0.7247],\n",
            "         [1.3722, 1.4697, 2.7334, 1.8557]],\n",
            "\n",
            "        [[0.7202, 1.7190, 0.6937, 1.7407],\n",
            "         [0.4594, 0.9283, 0.5929, 0.5578]]])\n",
            "\n",
            "Here is the result of batched matrix multiply with bmm:\n",
            "tensor([[[0.9347, 0.7368, 1.1844, 1.1910],\n",
            "         [1.1043, 0.5611, 0.8835, 0.9876]],\n",
            "\n",
            "        [[0.7110, 0.8649, 1.5978, 0.7247],\n",
            "         [1.3722, 1.4697, 2.7334, 1.8557]],\n",
            "\n",
            "        [[0.7202, 1.7190, 0.6937, 1.7407],\n",
            "         [0.4594, 0.9283, 0.5929, 0.5578]]])\n",
            "\n",
            "Difference: 2.384185791015625e-07\n",
            "Difference within threshold: True\n"
          ]
        }
      ],
      "source": [
        "B, N, M, P = 3, 2, 5, 4\n",
        "x = torch.rand(B, N, M)  # Random tensor of shape (B, N, M)\n",
        "y = torch.rand(B, M, P)  # Random tensor of shape (B, M, P)\n",
        "\n",
        "# We can use a for loop to (inefficiently) compute a batch of matrix multiply\n",
        "# operations\n",
        "z1 = torch.empty(B, N, P)  # Empty tensor of shape (B, N, P)\n",
        "for i in range(B):\n",
        "  z1[i] = x[i].mm(y[i])\n",
        "print('Here is the result of batched matrix multiply with a loop:')\n",
        "print(z1)\n",
        "\n",
        "z2 = None\n",
        "##############################################################################\n",
        "# TODO: Use bmm to compute a batched matrix multiply between x and y; store  #\n",
        "# the result in z2.                                                          #\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "z2 = torch.bmm(x,y)\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "print('\\nHere is the result of batched matrix multiply with bmm:')\n",
        "print(z2)\n",
        "\n",
        "# The two may not return exactly the same result; different linear algebra\n",
        "# routines often return slightly different results due to the fact that\n",
        "# floating-point math is non-exact and non-associative.\n",
        "diff = (z1 - z2).abs().max().item()\n",
        "print('\\nDifference:', diff)\n",
        "print('Difference within threshold:', diff < 1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UISn2pcf9QjY"
      },
      "source": [
        "## Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTj6f8VN9UZg"
      },
      "source": [
        "广播（Broadcasting）是一种强大的机制，使得PyTorch在进行算术运算时能够处理不同形状的数组。通常我们有一个较小的张量和一个较大的张量，我们希望多次使用较小的张量对较大的张量执行某些操作。\n",
        "\n",
        "例如，假设我们想将一个常量向量加到张量的每一行上。我们可以这样做："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF0Dhzlu9fef",
        "outputId": "335ab061-5d46-451b-f216-3c04a3352f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before:\n",
            " tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9],\n",
            "        [10, 11, 12]])\n",
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ]
        }
      ],
      "source": [
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
        "print(\"Before:\\n\",x)\n",
        "v = torch.tensor([1, 0, 1])\n",
        "y = torch.zeros_like(x)   # Create an empty matrix with the same shape as x\n",
        "\n",
        "# Add the vector v to each row of the matrix x with an explicit loop\n",
        "for i in range(4):\n",
        "    y[i, :] = x[i, :] + v #\n",
        "\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXpoBKE9vp7"
      },
      "source": [
        "此方法可行；然而，当张量 x 非常大时，在 Python 中显式地计算循环可能会很慢。请注意，将向量 v 加到张量 x 的每一行上，等同于通过垂直堆叠 v 的多个副本来形成张量 vv，然后对 x 和 vv 进行逐元素求和。我们可以这样实现这种方法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2_5cKeu94c2",
        "outputId": "dc886928-de88-463e-e94a-d2f69916d551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 1],\n",
            "        [1, 0, 1],\n",
            "        [1, 0, 1],\n",
            "        [1, 0, 1]])\n"
          ]
        }
      ],
      "source": [
        "vv = v.repeat((4, 1))  # Stack 4 copies of v on top of each other\n",
        "print(vv)              # Prints \"[[1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KiRj23p-QIs",
        "outputId": "6c803aa0-adc1-4ac7-f63f-64bd75ffd95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ]
        }
      ],
      "source": [
        "y = x + vv  # Add x and vv elementwise\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7NNlSsHBKib"
      },
      "source": [
        "PyTorch的广播机制使我们能够在不实际创建v的多个副本的情况下执行此计算。考虑这样使用广播机制："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jIiZc-ABBnt",
        "outputId": "b993677e-3830-4755-ad1b-f63d1e14fb36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ]
        }
      ],
      "source": [
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
        "v = torch.tensor([1, 0, 1])\n",
        "y = x + v  # Add v to each row of x using broadcasting\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuUBX8YnBSIG"
      },
      "source": [
        "尽管 x 的形状是 (4, 3) 而 v 的形状是 (3,)，由于广播机制，直线 y = x + v 仍然有效；这条直线的工作原理就像是 v 实际上具有形状 (4, 3)，其中每一行都是 v 的副本，并且求和是逐元素进行的。\n",
        "\n",
        "广播两个张量遵循以下规则：\n",
        "\n",
        "1. 如果张量的秩不同，则在较低秩张量的形状前添加1，直到两者的形状长度相同。\n",
        "\n",
        "2. 如果两个张量在某维度上的大小相同，或者其中一个张量在该维度上的大小为1，则称这两个张量在该维度上是*兼容*的。\n",
        "\n",
        "3. 如果两个张量在所有维度上都兼容，则它们可以一起广播。\n",
        "\n",
        "4. 广播后，每个张量的行为就好像它的形状等于两个输入张量形状的逐元素最大值。\n",
        "\n",
        "5. 在任意一个张量大小为1而另一个张量大小大于1的维度上，第一个张量的行为就好像它在该维度上被复制了一样。\n",
        "\n",
        "If this explanation does not make sense, try reading the explanation from the [documentation](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
        "\n",
        "Not all functions support broadcasting. You can find functions that does not support broadcasting from the official docs. (e.g. [`torch.mm`](https://pytorch.org/docs/stable/torch.html#torch.mm) does not support broadcasting, but [`torch.matmul`](https://pytorch.org/docs/1.1.0/torch.html#torch.matmul) does)\n",
        "\n",
        "Broadcasting can let us easily implement many different operations. For example we can compute an outer product of vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W-k7-hpCwlT",
        "outputId": "716b2d53-d870-4d1c-ef97-59f0ab77b5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4,  5],\n",
            "        [ 8, 10],\n",
            "        [12, 15]])\n"
          ]
        }
      ],
      "source": [
        "# Compute outer product of vectors\n",
        "v = torch.tensor([1, 2, 3])  # v has shape (3,)\n",
        "w = torch.tensor([4, 5])     # w has shape (2,)\n",
        "# To compute an outer product, we first reshape v to be a column\n",
        "# vector of shape (3, 1); we can then broadcast it against w to yield\n",
        "# an output of shape (3, 2), which is the outer product of v and w:\n",
        "# 要计算外积，我们首先将 v 重塑为形状为 (3, 1) 的列向量；然后我们可以将其与 w 进行广播，得到形状为 (3, 2) 的输出，这就是 v 和 w 的外积，这两个张量可以广播，依照的是上面的规则2：\n",
        "print(v.view(3, 1) * w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a9EcX20moP_"
      },
      "source": [
        "We can add a vector to each row of a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bhmBiwcDF1B",
        "outputId": "41fc5eed-b805-43fc-b24a-6824372c6374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the matrix:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Here is the vector:\n",
            "tensor([1, 2, 3])\n",
            "\n",
            "Add the vector to each row of the matrix:\n",
            "tensor([[2, 4, 6],\n",
            "        [5, 7, 9]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "v = torch.tensor([1, 2, 3])               # v has shape (3,)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(v)\n",
        "\n",
        "# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n",
        "# giving the following matrix:\n",
        "print('\\nAdd the vector to each row of the matrix:')\n",
        "print(x + v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYloJIvmm_Me"
      },
      "source": [
        "We can add a vector to each column of a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDTFKACqDK22",
        "outputId": "493815a9-6bee-4de0-9b2c-4589fd64118f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the matrix:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Here is the vector:\n",
            "tensor([4, 5])\n",
            "\n",
            "Add the vector to each column of the matrix:\n",
            "tensor([[ 5,  6,  7],\n",
            "        [ 9, 10, 11]])\n",
            "tensor([[ 5,  6,  7],\n",
            "        [ 9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "w = torch.tensor([4, 5])                  # w has shape (2,)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(w)\n",
        "\n",
        "# x has shape (2, 3) and w has shape (2,). We reshape w to (2, 1);\n",
        "# then when we add the two the result broadcasts to (2, 3):\n",
        "print('\\nAdd the vector to each column of the matrix:')\n",
        "print(x + w.view(-1, 1))\n",
        "\n",
        "# Another solution is the following:\n",
        "# 1. Transpose x so it has shape (3, 2)\n",
        "# 2. Since w has shape (2,), adding will broadcast to (3, 2)\n",
        "# 3. Transpose the result, resulting in a shape (2, 3)\n",
        "print((x.t() + w).t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9717YmBBpBfr"
      },
      "source": [
        "Multiply a tensor by a set of constants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UjWDp_XDc_-",
        "outputId": "e5b26e14-78af-4f47-9fef-b8ec512f0d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the matrix:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Here is the vector:\n",
            "tensor([  1,  10,  11, 100])\n",
            "\n",
            "Multiply x by a set of constants:\n",
            "tensor([[[  1,   2,   3],\n",
            "         [  4,   5,   6]],\n",
            "\n",
            "        [[ 10,  20,  30],\n",
            "         [ 40,  50,  60]],\n",
            "\n",
            "        [[ 11,  22,  33],\n",
            "         [ 44,  55,  66]],\n",
            "\n",
            "        [[100, 200, 300],\n",
            "         [400, 500, 600]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "c = torch.tensor([1, 10, 11, 100])        # c has shape (4)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(c)\n",
        "\n",
        "# 执行以下操作：\n",
        "# 1. 将 c 从形状 (4,) 重塑为 (4, 1, 1)\n",
        "# 2. x 的形状为 (2, 3)。由于它们的秩不同，当我们相乘时，x 的形状被自动调整为 (1, 2, 3)\n",
        "# 3. 形状为 (4, 1, 1) 和 (1, 2, 3) 的张量进行广播乘法的结果形状为 (4, 2, 3)\n",
        "# 4. 结果 y 的形状为 (4, 2, 3)，并且 y[i]（形状为 (2, 3)）等于 c[i] * x\n",
        "y = c.view(-1, 1, 1) * x\n",
        "print('\\nMultiply x by a set of constants:')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2EHXFBFq1ea"
      },
      "source": [
        "Your turn: 编写一个函数，用于标准化矩阵的列。该函数应计算每列的均值和标准差，然后对每列中的每个元素减去均值并除以标准差。\n",
        "\n",
        "Example:\n",
        "```\n",
        "x = [[ 0,  30,  600],\n",
        "     [ 1,  10,  200],\n",
        "     [-1,  20,  400]]\n",
        "```\n",
        "- The first column has mean 0 and std 1\n",
        "- The second column has mean 20 and std 10\n",
        "- The third column has mean 400 and std 200\n",
        "\n",
        "After normalizing the columns, the result should be:\n",
        "```\n",
        "y = [[ 0,  1,  1],\n",
        "     [ 1, -1, -1],\n",
        "     [-1,  0,  0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRAkY2tks1o_"
      },
      "outputs": [],
      "source": [
        "def normalize_columns(x):\n",
        "  \"\"\"\n",
        "  Normalize the columns of a matrix by subtracting the mean and dividing by the\n",
        "  standard deviation.\n",
        "\n",
        "  Inputs:\n",
        "  - x: Tensor of shape (N, M)\n",
        "\n",
        "  Returns:\n",
        "  - y: Tensor of shape (N, M) which is a copy of x with normalized columns.\n",
        "  \"\"\"\n",
        "  y = x.clone()\n",
        "  ##############################################################################\n",
        "  # TODO: Complete the implementation of this function. Do not modify x.       #\n",
        "  # Your implementation should not use any loops; instead you should use       #\n",
        "  # reduction and broadcasting operations.                                     #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  mean = y.mean(dim=0,keepdim=True)\n",
        "  std = y.std(dim=0,keepdim=True)\n",
        "  y = (y - mean) / std\n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHFR05ZXt4j5"
      },
      "source": [
        "Now test your implementation with a simple test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVh1DMqMr3zl",
        "outputId": "07074faf-5c60-4847-d21a-a88e9e25f105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is x0:\n",
            "tensor([[  0.,  30., 600.],\n",
            "        [  1.,  10., 200.],\n",
            "        [ -1.,  20., 400.]])\n",
            "Here is y0:\n",
            "tensor([[ 0.,  1.,  1.],\n",
            "        [ 1., -1., -1.],\n",
            "        [-1.,  0.,  0.]])\n"
          ]
        }
      ],
      "source": [
        "x0 = torch.tensor([[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]])\n",
        "y0 = normalize_columns(x0)\n",
        "print('Here is x0:')\n",
        "print(x0)\n",
        "print('Here is y0:')\n",
        "print(y0)\n",
        "assert y0.tolist() == [[0., 1., 1.], [1., -1., -1.], [-1., 0., 0.]]\n",
        "assert x0.tolist() == [[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7AZOedigszw"
      },
      "source": [
        "**解析**\n",
        "\n",
        "以本案例为例， y 的 shape 为 `torch.Size([3, 3])`\n",
        "\n",
        "```python\n",
        "mean = y.mean(dim=0,keepdim=True) # tensor([0,20,400])\n",
        "std = y.std(dim=0,keepdim=True) # tensor([1,10,200])\n",
        "```\n",
        "\n",
        "这两个都是 `torch.Size([1, 3])` 的张量，与 y 进行运算时，他们都会自动广播为 `torch.Size([3, 3])`，广播后的张量为\n",
        "\n",
        "```python\n",
        "# mean\n",
        "tensor([\n",
        "    [0,20,400],\n",
        "    [0,20,400],\n",
        "    [0,20,400]\n",
        "    ])\n",
        "\n",
        "#std\n",
        "tensor([\n",
        "    [1,10,200],\n",
        "    [1,10,200],\n",
        "    [1,10,200]\n",
        "    ])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN6FfqU9wFeG"
      },
      "source": [
        "## Running on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds6SDTbrwOc1"
      },
      "source": [
        "PyTorch最重要的特性之一是它能够利用图形处理单元（GPU）来加速其张量操作。\n",
        "\n",
        "我们可以轻松检查PyTorch是否配置为使用GPU：\n",
        "\n",
        "张量可以使用`.to`方法移动到任何设备上。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_RkoFEVVKWlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3281f006-5941-4f86-c1de-68afda094cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch can use GPUs!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "  print('PyTorch can use GPUs!')\n",
        "else:\n",
        "  print('PyTorch cannot use GPUs.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i_5n_XuKr5k"
      },
      "source": [
        "你可以在Colab中通过以下路径启用GPU：运行时 -> 更改运行时类型 -> 硬件加速器 -> GPU。\n",
        "\n",
        "这可能会导致Colab运行时重启，因此我们将在下一个单元格中重新导入torch。\n",
        "\n",
        "我们已经知道PyTorch张量有一个`dtype`属性，用于指定其数据类型。所有PyTorch张量还有一个`device`属性，用于指定张量存储的设备——可以是CPU，或者是CUDA（适用于NVIDIA GPU）。位于CUDA设备上的张量将自动使用该设备来加速其所有操作。\n",
        "\n",
        "与数据类型一样，我们可以使用 [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) 方法来更改张量的设备。我们还可以使用便捷的 `.cuda()` 和 `.cpu()` 方法在CPU和GPU之间移动张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D03s614dMCvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab039a7-5fd9-47bc-d2d6-cb6ba849dd78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0 device: cpu\n",
            "x1 device: cuda:0\n",
            "x2 device: cuda:0\n",
            "x3 device: cpu\n",
            "x4 device: cpu\n",
            "y device / dtype: cuda:0 torch.float64\n",
            "x5 device / dtype: cuda:0 torch.float64\n"
          ]
        }
      ],
      "source": [
        "# Construct a tensor on the CPU\n",
        "x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "print('x0 device:', x0.device)\n",
        "\n",
        "# Move it to the GPU using .to()\n",
        "x1 = x0.to('cuda')\n",
        "print('x1 device:', x1.device)\n",
        "\n",
        "# Move it to the GPU using .cuda()\n",
        "x2 = x0.cuda()\n",
        "print('x2 device:', x2.device)\n",
        "\n",
        "# Move it back to the CPU using .to()\n",
        "x3 = x1.to('cpu')\n",
        "print('x3 device:', x3.device)\n",
        "\n",
        "# Move it back to the CPU using .cpu()\n",
        "x4 = x2.cpu()\n",
        "print('x4 device:', x4.device)\n",
        "\n",
        "# We can construct tensors directly on the GPU as well\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n",
        "print('y device / dtype:', y.device, y.dtype)\n",
        "\n",
        "# Calling x.to(y) where y is a tensor will return a copy of x with the same\n",
        "# device and dtype as y\n",
        "x5 = x0.to(y)\n",
        "print('x5 device / dtype:', x5.device, x5.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-TDxICdOmJo"
      },
      "source": [
        "在GPU上执行大型张量操作比在CPU上运行等效操作要**快得多**。\n",
        "\n",
        "这里我们比较在CPU和GPU上对两个形状为(10000, 10000)的张量进行相加的速度：\n",
        "\n",
        "（注意，GPU代码可能与CPU代码异步运行，因此在计时GPU操作速度时，使用`torch.cuda.synchronize`来同步CPU和GPU非常重要。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GW14ZF-_PK7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30bb1447-094c-4288-a5be-c4b685b0c5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max difference between c_gpu and c_cpu: 0.0\n",
            "CPU time: 268.42 ms\n",
            "GPU time: 55.88 ms\n",
            "GPU speedup: 4.80 x\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "a_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "b_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "\n",
        "a_gpu = a_cpu.cuda()\n",
        "b_gpu = b_cpu.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "t0 = time.time()\n",
        "c_cpu = a_cpu + b_cpu\n",
        "t1 = time.time()\n",
        "c_gpu = a_gpu + b_gpu\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time()\n",
        "\n",
        "# Check that they computed the same thing\n",
        "diff = (c_gpu.cpu() - c_cpu).abs().max().item()\n",
        "print('Max difference between c_gpu and c_cpu:', diff)\n",
        "\n",
        "cpu_time = 1000.0 * (t1 - t0)\n",
        "gpu_time = 1000.0 * (t2 - t1)\n",
        "print('CPU time: %.2f ms' % cpu_time)\n",
        "print('GPU time: %.2f ms' % gpu_time)\n",
        "print('GPU speedup: %.2f x' % (cpu_time / gpu_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HEAVPEwviYb"
      },
      "source": [
        "你应该注意到，在GPU上运行相同的计算比在CPU上快了30多倍！由于GPU提供了巨大的加速，我们将从作业2开始使用GPU来加速大部分机器学习代码。\n",
        "\n",
        "轮到你了：使用GPU来加速以下矩阵乘法操作。通过使用GPU，你应该能看到大约10倍的加速。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uqEUdst7SAuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cd11e9-fe43-4703-a80c-e7964b1a8d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y1 on CPU: True\n",
            "Max difference between y0 and y1: 0.001220703125\n",
            "Difference within tolerance: True\n",
            "CPU time: 151.09 ms\n",
            "GPU time: 30.87 ms\n",
            "GPU speedup: 4.89 x\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "x = torch.rand(512, 4096)\n",
        "w = torch.rand(4096, 4096)\n",
        "\n",
        "t0 = time.time()\n",
        "y0 = x.mm(w)\n",
        "t1 = time.time()\n",
        "\n",
        "y1 = None\n",
        "##############################################################################\n",
        "# TODO: Write a bit of code that performs matrix multiplication of x and w   #\n",
        "# on the GPU, and then moves the result back to the CPU. Store the result    #\n",
        "# in y1.\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "x_gpu = x.to('cuda')\n",
        "w_gpu = w.to('cuda')\n",
        "y1 = x_gpu.mm(w_gpu)\n",
        "y1 = y1.to('cpu')\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time()\n",
        "\n",
        "print('y1 on CPU:', y1.device == torch.device('cpu'))\n",
        "diff = (y0 - y1).abs().max().item()\n",
        "print('Max difference between y0 and y1:', diff)\n",
        "print('Difference within tolerance:', diff < 5e-2)\n",
        "\n",
        "cpu_time = 1000.0 * (t1 - t0)\n",
        "gpu_time = 1000.0 * (t2 - t1)\n",
        "print('CPU time: %.2f ms' % cpu_time)\n",
        "print('GPU time: %.2f ms' % gpu_time)\n",
        "print('GPU speedup: %.2f x' % (cpu_time / gpu_time))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hQrEwOpXb9Gh",
        "zjosrOn8mOMV",
        "OgPaSNS2mVPn",
        "zeH5501nmh7W",
        "HrBSx6hYu8ca",
        "LWagwmXuvIle",
        "Yz_VDA3IvP33",
        "Rz_hiJD33fu1",
        "rlANfnILvX3S",
        "mo-PoTWNvbba",
        "4y93rPhGveWw",
        "oGt8ZPb_vixw",
        "Ad-xqELwyqpN",
        "Ql9_eXuU4OG8",
        "Z150qBob4Wkz",
        "f4SJCVbf-bZ0",
        "WJiiBxNE-X8g",
        "NgcdvD1evxTQ",
        "1BCVlPHZ4_Qz",
        "yDyH9USAuyZ-",
        "lRyLyXU2u29N",
        "UISn2pcf9QjY",
        "uN6FfqU9wFeG"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}